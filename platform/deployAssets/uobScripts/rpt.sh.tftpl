#!/bin/bash
set -euxo pipefail

exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1

HOST_NAME=${host_name}
EFS_NAME=${efs_name}
MOUNT_POINT=${mount_point}
FI_NAME=${fi_name}
SUB_DOMAIN=${sub_domain}
PARAMETER_NAME=${parameter_name}
SSH_KEY_FILE_NAME=${ssh_key_file_name}
HOST_FQDN="$HOST_NAME.$SUB_DOMAIN"
BLK_DEVICE_NAME=${blk_device_name}
KEY_PAIR_NAME=${key_pair_name}
PUBLIC_KEY_FILE_NAME=${public_key_file_name}
VPC_CIDR=${vpc_cidr}
INGRESS_VPC_CIDR=${ingress_vpc_cidr}
USRRPT_PW=${usrrptPw}
# This curl command below uses the IMDSv1 and was not updated as part of ticket AE-485 because this userdata will no longer be used on the servers. 
# Going forward if there is a change that needs to happen we will use ansible instead. If this does need to be used than this will 
# need to be updated to follow the IMDSv2 metadata commands found in the aws docs here - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-dynamic-data-retrieval.html
REGION=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | grep region | awk -F\" '{print $4}')
EFS_ID=$(aws efs describe-file-systems --query "FileSystems[?Name=='$EFS_NAME'].FileSystemId" --output text --region $REGION)
IP_ADDRESS=$(hostname -I)
KEY_PAIR=$(aws ec2 describe-key-pairs --key-names $KEY_PAIR_NAME --query "KeyPairs[0].KeyName" --output text)
PUBLIC_KEY=$(aws ec2 describe-key-pairs --key-names $KEY_PAIR_NAME --query "KeyPairs[0].PublicKey" --output text --include-public-key)
SSH_KEY=$(aws ssm get-parameter --name $PARAMETER_NAME --with-decryption --query "Parameter.Value" --output text)
FS_DIRECTORIES="/home/ac$FI_NAME /home/usrrpt /platform"
PARTITION_SIZES="50G 50G 100G"
NEW_PATH="/usr/local/bin"
DD_USER="dd-agent"
EFS_DIRECTORIES="
support
shares
depot
"

function backup_directory() {
    src_dir="$1"
    dest_dir="$2"

    if [ ! -d "$dest_dir" ]; then
        mkdir -p "$dest_dir"
    fi

    rsync -azP $src_dir $dest_dir    

    # If destination directory is in FS_DIRECTORIES,
    # restore selinux context of the destination directory and delete the src_dir
    if [[ "$FS_DIRECTORIES[*]" =~ "$dest_dir" ]]; then
        restorecon -Rv "$dest_dir"
        rm -rf "$src_dir"
    fi
}

function generate_partition_names() {
    local blk_device_name="$1"
    local num_partitions="$2"
    local partition_names=""

    for ((i = 1; i <= num_partitions; i++)); do
        partition_names+="${blk_device_name}p$i "
    done

    echo "$partition_names"
}

function enable_ssh_access() {
  local USERNAME="$1"
  mkdir /home/"$USERNAME"/.ssh
  chown "$USERNAME":"$USERNAME" /home/"$USERNAME"/.ssh
  chmod 700 /home/"$USERNAME"/.ssh
  cat "$PUBLIC_KEY_FILE_NAME" >> /home/"$USERNAME"/.ssh/authorized_keys
  chown "$USERNAME":"$USERNAME" /home/"$USERNAME"/.ssh/authorized_keys
  chmod 600 /home/"$USERNAME"/.ssh/authorized_keys
  echo "AllowUsers $USERNAME" | tee -a /etc/ssh/sshd_config
}

# Fix aws command permissions
chmod -R o+rx /usr/local/aws-cli/

# Update root user path
echo 'export PATH='"$NEW_PATH"':$PATH' | tee -a /root/.bashrc > /dev/null

# Set the hostname
echo "$HOST_NAME" > /etc/hostname
hostname -F /etc/hostname
echo "$IP_ADDRESS $HOST_FQDN $HOST_NAME" >> /etc/hosts

# Update resolv.conf file
echo "DOMAIN=$SUB_DOMAIN" >> /etc/sysconfig/network-scripts/ifcfg-eth0
systemctl restart NetworkManager

############################# User Settings BEGIN ####################################

# Create FI and Ansible users
useradd -m -s /bin/ksh ac$FI_NAME
useradd -m -s /bin/bash ansible


#Create UOB group
groupadd uob

#Add users to group
for user in ansible usrrpt ac$FI_NAME; do
    usermod -aG uob $user
done

# Enable ssh for ansible, ac$FI_NAME and usrrpt users
echo "$PUBLIC_KEY" > "$PUBLIC_KEY_FILE_NAME"

enable_ssh_access "ansible"
enable_ssh_access "ac$FI_NAME"
enable_ssh_access "usrrpt"

systemctl restart sshd

# Set password for USRRPT user
echo "usrrpt:$USRRPT_PW" | chpasswd

# enable password authentication
sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config

# Allow all system users to switch users
sed -i '/auth\s\+sufficient\s\+pam_rootok.so/a auth sufficient pam_permit.so' /etc/pam.d/su

# Allow uob group to run sudo commands
echo "%uob ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/uobgroup

############################# User Settings END ####################################

# Loop until the device is available
while true; do
    if lsblk -p -l | grep -q "$BLK_DEVICE_NAME"; then
        break
    else
        sleep 10
    fi
done
echo "Device $BLK_DEVICE_NAME is available"

# Create filesystem directories
for fs_directory in $FS_DIRECTORIES; do
    if [ ! -d "$fs_directory" ]; then
        mkdir -m 755 "$fs_directory"
    fi
done

# Check if the device has partitions
if blkid "$BLK_DEVICE_NAME" > /dev/null; then
    echo "Filesystem already exists on $BLK_DEVICE_NAME"
    PARTITION_NAMES=$(generate_partition_names "$BLK_DEVICE_NAME" "$(echo "$FS_DIRECTORIES" | wc -w)") 

    for fs_directory in $FS_DIRECTORIES; do
        partition_name=$(echo "$PARTITION_NAMES" | cut -d' ' -f1)
        PARTITION_NAMES=$(echo "$PARTITION_NAMES" | cut -d' ' -f2-)
        mount "$partition_name" "/$fs_directory"
        restorecon -Rv "$fs_directory"
    done
    # Check for known_hosts file
    for user in ac$FI_NAME usrrpt; do
        if [ -f /home/$user/.ssh/known_hosts ]; then
            # Delete contents of known hosts file
            echo "" > /home/$user/.ssh/known_hosts
        fi
    done
else
    echo "$BLK_DEVICE_NAME does not have partitions"

    backup_directory /home/ac$FI_NAME/ /tmp/ac$FI_NAME
    backup_directory /home/usrrpt/ /tmp/usrrpt

    # Clear all partition data
    sgdisk -o "$BLK_DEVICE_NAME"

    # Create partition table and partitions
    counter=1
    for partition_size in $PARTITION_SIZES; do
        sgdisk -n "$counter":0:"+$partition_size" -t "$counter":8e00 "$BLK_DEVICE_NAME"
        ((counter++))
    done

    # Inform the kernel of the new partition table(s)
    partprobe "$BLK_DEVICE_NAME"

    # Create filesystems and mount partitions
    PARTITION_NAMES="$(generate_partition_names "$BLK_DEVICE_NAME" "$(echo "$FS_DIRECTORIES" | wc -w)")"

    for fs_directory in $FS_DIRECTORIES; do
        partition_name=$(echo "$PARTITION_NAMES" | cut -d' ' -f1)
        PARTITION_NAMES=$(echo "$PARTITION_NAMES" | cut -d' ' -f2-)
        mkfs -t xfs "$partition_name"
        mount "$partition_name" "/$fs_directory"
        echo "$partition_name /$fs_directory xfs defaults 0 0" >> /etc/fstab
    done

    backup_directory /tmp/ac$FI_NAME/ /home/ac$FI_NAME
    backup_directory /tmp/usrrpt/ /home/usrrpt

    # Create directories
    DIRECTORIES="
    /platform/actuate11
    /platform/actuate11/AcServer
    /platform/$FI_NAME/actuate
    "

    for directory in $DIRECTORIES; do
    if [ ! -d "$directory" ]; then
        mkdir -p "$directory"
        chmod 755 "$directory"
    fi
    done

    chown -R ac$FI_NAME:ac$FI_NAME /platform/

    setfacl -m g:uob:rwx /platform /opt /tmp /var 
fi

# Check if mount point exists, if not create mount point
if [ ! -d "$MOUNT_POINT" ]; then
    mkdir $MOUNT_POINT
    EFS_ROOT=$(dirname $MOUNT_POINT)
    chmod -R 777 $EFS_ROOT
fi

# Wait for DNS resolution before moving on
while true; do
    if nslookup $EFS_ID.efs.$REGION.amazonaws.com > /dev/null 2>&1; then
        echo "DNS resolution successful"
        break
    else
        echo "Waiting for DNS resolution..."
        sleep 10
    fi
done

# Kill nfs-idmapd.service as we don't want EFS enforcing uid/gid mapping/checks
# check if nfs-idmapd.service is running, if so kill it
if systemctl is-active --quiet nfs-idmapd.service; then
    systemctl stop nfs-idmapd.service
fi
# Beat the service to death
systemctl disable nfs-idmapd.service
systemctl mask nfs-idmapd.service
systemctl daemon-reload
systemctl restart nfs-utils.service

# Mount the EFS file system to the EC2 instance
mount -t efs -o tls $EFS_ID:/ $MOUNT_POINT && \
echo "EFS file system '$EFS_NAME' was successfully mounted to '$MOUNT_POINT'" || \
{ echo "Failed to mount EFS file system '$EFS_NAME'"; exit 1; }

#Check if RPT EFS directories exists, if not create RPT EFS directories
for directory in $EFS_DIRECTORIES; do
    if [ ! -d "$MOUNT_POINT/$directory" ]; then
        mkdir -p $MOUNT_POINT/$directory
        chmod 777 $MOUNT_POINT/$directory
    fi
done

# Create /support /depot and /shares directories if they don't exist and create symbolic links to APP EFS directories
for directory in support depot shares; do
    if [ ! -d "/$directory" ]; then
        mkdir -m 777 /$directory
        ln -sfn $MOUNT_POINT/$directory/* /$directory
    fi
done

#Create .profile for FI user and change ownership
touch /home/ac$FI_NAME/.profile
chown ac$FI_NAME:ac$FI_NAME /home/ac$FI_NAME/.profile
echo "Created .profile for ac$FI_NAME"

#Update FI user .profile
tee /home/ac$FI_NAME/.profile > /dev/null <<'EOF'
export TERM=vt100
set -o vi
export EDITOR=vi
export VISUAL=vi
export FCEDIT=vi

CHECKUSER=`id -u`
CHECKGROUP=`id -g`
if [ $CHECKUSER = "0" ] || [ CHECKGROUP = "0" ]
  then PROMPT="#"
else
  PROMPT="$"
fi

OSTYPE=`uname -a|awk '{print $1}'`
export OSTYPE

case $OSTYPE in
SunOS)
        export SHELL=/bin/ksh
        USERNAME=`/usr/xpg4/bin/id -u -n`
        export USERNAME
        PS1='$USERNAME@'`hostname`':$PWD$PROMPT'
        export PS1
        echo $USERNAME > /dev/null
        ;;
AIX|Linux)
        USERNAME=`/usr/bin/id -u -n`
        export USERNAME
        PS1='$USERNAME@'`hostname`':$PWD$PROMPT'
        export PS1
        echo $USERNAME > /dev/null
        ;;
esac

stty susp ^Z
stty intr ^C
stty erase ^?

umask 002


# Alias to dispaly running processes for the user
alias pc='ps -ef|grep -v grep|grep $LOGNAME'



export AC_SERVER_HOME=/opt/actuate11/AcServer

if [ -d $AC_SERVER_HOME ]
then
  cd $AC_SERVER_HOME
fi

alias stopactuate="cd $AC_SERVER_HOME/bin && ./shutdown_srvr.sh -y -t 0"
alias startactuate="cd $AC_SERVER_HOME/bin && ./start_srvr.sh"
EOF

echo "Updated .profile for ac$FI_NAME"

# Datadog deploy time logging configurations
setfacl -m "u:$DD_USER:rx" /var/log/messages
setfacl -m "u:$DD_USER:rx" /var/log/secure
setfacl -Rm "u:$DD_USER:rx" /var/log/audit
usermod -a -G systemd-journal "$DD_USER"

# Datadog deploy time network configurations
touch system_probe_policy.te
sudo tee system_probe_policy.te >> /dev/null <<'EOF'
module system_probe_policy 1.0;

type system_probe_t;
require {
  attribute file_type, exec_type, entry_type, base_file_type, base_ro_file_type, non_auth_file_type, non_security_file_type;
  class bpf { map_create map_read map_write prog_load prog_run };
}

typeattribute system_probe_t file_type, exec_type, entry_type, base_file_type, base_ro_file_type, non_auth_file_type, non_security_file_type;
allow system_probe_t self:bpf { map_create map_read map_write prog_load prog_run };
EOF
checkmodule -M -m -o system_probe_policy.mod system_probe_policy.te
semodule_package -o system_probe_policy.pp -m system_probe_policy.mod
semodule -v -i system_probe_policy.pp
semanage fcontext -a -t system_probe_t /opt/datadog-agent/embedded/bin/system-probe
semanage fcontext -d -t system_probe_t /opt/datadog-agent/embedded/bin/system-probe
restorecon -v /opt/datadog-agent/embedded/bin/system-probe
sudo systemctl enable datadog-agent-sysprobe
sudo systemctl start datadog-agent-sysprobe
sudo systemctl restart datadog-agent

# Datadog enable APM | Will move to base image
mkdir -m 777 /tracer
wget -O /tracer/dd-java-agent.jar https://dtdg.co/latest-java-tracer
chown dd-agent:dd-agent /tracer/dd-java-agent.jar
chmod 777 /tracer/dd-java-agent.jar

# Allow All VPC traffic to the instance | Will update once we get a list of ports from Devs
# Temp disable Selinux because its blocking firewalld changes
sudo setenforce 0
firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address='"$VPC_CIDR"' accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family="ipv4" source address='"$VPC_CIDR"' port port="1-65535" protocol="tcp" accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family="ipv4" source address='"$VPC_CIDR"' port port="1-65535" protocol="udp" accept'

firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address='"$INGRESS_VPC_CIDR"' accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family="ipv4" source address='"$INGRESS_VPC_CIDR"' port port="1-65535" protocol="tcp" accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family="ipv4" source address='"$INGRESS_VPC_CIDR"' port port="1-65535" protocol="udp" accept'

firewall-cmd --reload
sudo setenforce 1

# delete the temporary files
rm -f system_probe_policy.te system_probe_policy.mod system_probe_policy.pp $PUBLIC_KEY_FILE_NAME

echo END