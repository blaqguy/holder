#!/bin/bash
set -euxo pipefail

exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1
echo "Logging to /var/log/user-data.log..."

HOST_NAME=${host_name}
EFS_NAME=${efs_name}
MOUNT_POINT=${mount_point}
FI_NAME=${fi_name}
SUB_DOMAIN=${sub_domain}
PARAMETER_NAME=${parameter_name}
SSH_KEY_FILE_NAME=${ssh_key_file_name}
HOST_FQDN="$HOST_NAME.$SUB_DOMAIN"
BLK_DEVICE_NAME=${blk_device_name}
KEY_PAIR_NAME=${key_pair_name}
PUBLIC_KEY_FILE_NAME=${public_key_file_name}
VPC_CIDR=${vpc_cidr}
INGRESS_VPC_CIDR=${ingress_vpc_cidr}
# This curl command below uses the IMDSv1 and was not updated as part of ticket AE-485 because this userdata will no longer be used on the servers. 
# Going forward if there is a change that needs to happen we will use ansible instead. If this does need to be used than this will 
# need to be updated to follow the IMDSv2 metadata commands found in the aws docs here - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-dynamic-data-retrieval.html
REGION=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | grep region | awk -F\" '{print $4}')
EFS_ID=$(aws efs describe-file-systems --query "FileSystems[?Name=='$EFS_NAME'].FileSystemId" --output text --region $REGION)
IP_ADDRESS=$(hostname -I)
KEY_PAIR=$(aws ec2 describe-key-pairs --key-names $KEY_PAIR_NAME --query "KeyPairs[0].KeyName" --output text)
PUBLIC_KEY=$(aws ec2 describe-key-pairs --key-names $KEY_PAIR_NAME --query "KeyPairs[0].PublicKey" --output text --include-public-key)
SSH_KEY=$(aws ssm get-parameter --name $PARAMETER_NAME --with-decryption --query "Parameter.Value" --output text)
NEW_PATH="/usr/local/bin"
DD_USER="dd-agent"

function generate_partition_names() {
    local blk_device_name="$1"
    local num_partitions="$2"
    local partition_names=""

    for ((i = 1; i <= num_partitions; i++)); do
        partition_names+="${blk_device_name}p$i "
    done

    echo "$partition_names"
}

function enable_ssh_access() {
  local USERNAME="$1"
  mkdir /home/"$USERNAME"/.ssh
  chown "$USERNAME":"$USERNAME" /home/"$USERNAME"/.ssh
  chmod 700 /home/"$USERNAME"/.ssh
  cat "$PUBLIC_KEY_FILE_NAME" >> /home/"$USERNAME"/.ssh/authorized_keys
  chown "$USERNAME":"$USERNAME" /home/"$USERNAME"/.ssh/authorized_keys
  chmod 600 /home/"$USERNAME"/.ssh/authorized_keys
  echo "AllowUsers $USERNAME" | tee -a /etc/ssh/sshd_config
}

# Fix aws command permissions
sudo chmod -R o+rx /usr/local/aws-cli/

# Update root user path
echo 'export PATH='"$NEW_PATH"':$PATH' | tee -a /root/.bashrc > /dev/null

# Set the hostname
echo "$HOST_NAME" > /etc/hostname
hostname -F /etc/hostname
echo "$IP_ADDRESS $HOST_FQDN $HOST_NAME" >> /etc/hosts

# Update resolv.conf file
echo "DOMAIN=$SUB_DOMAIN" >> /etc/sysconfig/network-scripts/ifcfg-eth0
systemctl restart NetworkManager

# Create Ansible user
useradd -m -s /bin/bash ansible

# Create UOB group
groupadd uob

#Add users to group
for user in ansible; do
    usermod -aG uob $user
done

# Enable ssh access for ansible user
echo "$PUBLIC_KEY" > "$PUBLIC_KEY_FILE_NAME"

enable_ssh_access "ansible"
systemctl restart sshd

# Allow all system users to switch users
sed -i '/auth\s\+sufficient\s\+pam_rootok.so/a auth sufficient pam_permit.so' /etc/pam.d/su

# Allow uob group to run sudo commands
echo "%uob ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/uobgroup

# Loop until the device is available
while true; do
    if lsblk -p -l | grep -q "$BLK_DEVICE_NAME"; then
        break
    else
        sleep 10
    fi
done
echo "Device $BLK_DEVICE_NAME is available"

# if /platform doesn't exist, create it
if [ ! -d "/platform" ]; then
    mkdir -m 755 /platform
fi

# Check if the device has partitions
if blkid "$BLK_DEVICE_NAME" > /dev/null; then
    echo "Filesystem already exists on $BLK_DEVICE_NAME"
    mount /dev/nvme1n1 /platform
else
    echo "$BLK_DEVICE_NAME does not have partitions"

    # Create filesystem
    mkfs -t xfs $BLK_DEVICE_NAME

    # Mount the filesystem
    mount /dev/nvme1n1 /platform

    # Persist the mount
    echo "/dev/nvme1n1 /platform xfs defaults 0 0" >> /etc/fstab

    setfacl -m g:uob:rwx /platform /opt /tmp /var    

fi

# Check if mount point exists, if not create mount point
if [ ! -d "$MOUNT_POINT" ]; then
    mkdir $MOUNT_POINT
    EFS_ROOT=$(dirname $MOUNT_POINT)
    chmod -R 777 $EFS_ROOT
fi

# Wait for DNS resolution before moving on
while true; do
    if nslookup $EFS_ID.efs.$REGION.amazonaws.com > /dev/null 2>&1; then
        echo "DNS resolution successful"
        break
    else
        echo "Waiting for DNS resolution..."
        sleep 10
    fi
done

# Kill nfs-idmapd.service as we don't want EFS enforcing uid/gid mapping/checks
# check if nfs-idmapd.service is running, if so kill it
if systemctl is-active --quiet nfs-idmapd.service; then
    systemctl stop nfs-idmapd.service
fi
# Beat the service to death
systemctl disable nfs-idmapd.service
systemctl mask nfs-idmapd.service
systemctl daemon-reload
systemctl restart nfs-utils.service

# Mount the EFS file system to the EC2 instance
mount -t efs -o tls $EFS_ID:/ $MOUNT_POINT && \
echo "EFS file system '$EFS_NAME' was successfully mounted to '$MOUNT_POINT'" || \
{ echo "Failed to mount EFS file system '$EFS_NAME'"; exit 1; }

#Check if Luquibase EFS directories exists, if not create Liquibase EFS directories
for directory in depot; do
    if [ ! -d "$MOUNT_POINT/$directory" ]; then
        mkdir -m 777 $MOUNT_POINT/$directory
        echo "Created $MOUNT_POINT/$directory"
    fi
done

# Create /support /depot and /shares directories if they don't exist and create symbolic links to Ansible EFS directories
for directory in support depot shares; do
    if [ ! -d "/$directory" ]; then
        mkdir -m 777 /$directory
        ln -sfn $MOUNT_POINT/$directory/* /$directory
    fi
done

# Datadog deploy time logging configurations
setfacl -m "u:$DD_USER:rx" /var/log/messages
setfacl -m "u:$DD_USER:rx" /var/log/secure
setfacl -Rm "u:$DD_USER:rx" /var/log/audit
usermod -a -G systemd-journal "$DD_USER"

# Datadog deploy time network configurations
touch system_probe_policy.te
sudo tee -a system_probe_policy.te >> /dev/null <<'EOF'
module system_probe_policy 1.0;

type system_probe_t;
require {
  attribute file_type, exec_type, entry_type, base_file_type, base_ro_file_type, non_auth_file_type, non_security_file_type;
  class bpf { map_create map_read map_write prog_load prog_run };
}

typeattribute system_probe_t file_type, exec_type, entry_type, base_file_type, base_ro_file_type, non_auth_file_type, non_security_file_type;
allow system_probe_t self:bpf { map_create map_read map_write prog_load prog_run };
EOF
checkmodule -M -m -o system_probe_policy.mod system_probe_policy.te
semodule_package -o system_probe_policy.pp -m system_probe_policy.mod
semodule -v -i system_probe_policy.pp
semanage fcontext -a -t system_probe_t /opt/datadog-agent/embedded/bin/system-probe
semanage fcontext -d -t system_probe_t /opt/datadog-agent/embedded/bin/system-probe
restorecon -v /opt/datadog-agent/embedded/bin/system-probe
sudo systemctl enable datadog-agent-sysprobe
sudo systemctl start datadog-agent-sysprobe
sudo systemctl restart datadog-agent

# Allow All VPC traffic to the instance | Will update once we get a list of ports from Devs
# Temp disable Selinux because its blocking firewalld changes
sudo setenforce 0
firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address='"$VPC_CIDR"' accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family="ipv4" source address='"$VPC_CIDR"' port port="1-65535" protocol="tcp" accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family="ipv4" source address='"$VPC_CIDR"' port port="1-65535" protocol="udp" accept'

firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address='"$INGRESS_VPC_CIDR"' accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family="ipv4" source address='"$INGRESS_VPC_CIDR"' port port="1-65535" protocol="tcp" accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family="ipv4" source address='"$INGRESS_VPC_CIDR"' port port="1-65535" protocol="udp" accept'

firewall-cmd --reload
sudo setenforce 1

# delete the temporary files
rm -f system_probe_policy.te system_probe_policy.mod system_probe_policy.pp $PUBLIC_KEY_FILE_NAME

echo END